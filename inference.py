import torch
import argparse
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

from modeling_codepatch import CodePatchConfig, CodeEncoderConfig, CodePatchForConditionalGeneration
from processing_codepatch import CodePatchProcessor
from transformers import GemmaForCausalLM, GemmaConfig
from ast_parser.ast_utils import get_semantic_patches


def get_args():
    parser = argparse.ArgumentParser(description="Run inference with a trained CodePatch model.")
    parser.add_argument("--checkpoint_path", type=str, required=True, help="Path to the model checkpoint .pt file.")
    parser.add_argument("--code_file", type=str, required=True, help="Path to the MATLAB code file.")
    parser.add_argument("--max_new_tokens", type=int, default=200, help="Maximum number of new tokens to generate.")
    parser.add_argument("--temperature", type=float, default=0.7, help="Temperature for sampling.")
    parser.add_argument("--top_p", type=float, default=0.9, help="Top-p (nucleus) sampling.")
    parser.add_argument("--repetition_penalty", type=float, default=1.2, help="Penalty for repeating tokens.")
    parser.add_argument("--debug", action="store_true", help="Enable debug prints for inputs and per-token generation.")
    return parser.parse_args()

def sample_top_p(probs, p):
    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)
    probs_sum = torch.cumsum(probs_sort, dim=-1)
    mask = probs_sum - probs_sort > p
    probs_sort[mask] = 0.0
    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))
    next_token = torch.multinomial(probs_sort, num_samples=1)
    next_token = torch.gather(probs_idx, -1, next_token)
    return next_token

def main():
    args = get_args()
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    gemma_model_name = "google/gemma-2b"

    # --- 1. Re-create the exact model and processor configuration used for training ---
    code_encoder_config = CodeEncoderConfig(freeze_encoder=True)
    processor = CodePatchProcessor(
        "microsoft/codebert-base", gemma_model_name, code_encoder_config, device=device
    )
    # Load the official Gemma config to ensure architectures match
    text_config = GemmaConfig.from_pretrained(gemma_model_name)
    
    full_config = CodePatchConfig(
        code_encoder_config=vars(code_encoder_config), text_config=text_config.to_dict(),
        projection_dim=text_config.hidden_size, freeze_llm=True,
    )

    # --- 2. Instantiate the model and load the trained weights ---
    print("Instantiating model and loading REAL Gemma weights...")
    model = CodePatchForConditionalGeneration(full_config).to(device)
    
    # Load the pre-trained weights for the language model part FIRST
    gemma_model = GemmaForCausalLM.from_pretrained("google/gemma-2b", torch_dtype=torch.bfloat16)
    model.language_model.load_state_dict(gemma_model.state_dict())

    print(f"Loading checkpoint from: {args.checkpoint_path}")
    checkpoint = torch.load(args.checkpoint_path, map_location=device)
    model.multi_modal_projector.load_state_dict(checkpoint['projector_state_dict'])
    model.code_encoder.position_embedding.load_state_dict(checkpoint['pos_embedding_state_dict'])
    model.eval() # Set model to evaluation mode
    print("Checkpoint loaded successfully.")

    # --- 3. Prepare Inputs ---
    print(f"Loading code from: {args.code_file}")
    with open(args.code_file, "r") as f:
        code_string = f.read()
    
    # Summarize code for prompt grounding (simple: take first few lines)
    code_summary = ' '.join(code_string.splitlines()[:3])  # First 3 lines as summary
    prompt_text = f"The MATLAB code is: {code_summary}... Describe the plot generated by this code, including shape, min/max, and key features:"
    inputs = processor(code=code_string, text=prompt_text)
    
    if args.debug:
        # --- Debug Prints to Verify Inputs ---
        print("\n--- Input Debug ---")
        print(f"Code Input IDs Shape: {inputs['code_input_ids'].shape} (Batch, Num_Patches, Patch_Length)")
        valid_patches = (inputs['code_attention_mask'].any(dim=-1)).sum().item()  # Count non-padded patches
        print(f"Number of Valid (Non-Padded) Patches: {valid_patches} out of {inputs['code_input_ids'].shape[1]}")
        decoded_prompt = processor.text_tokenizer.decode(inputs['prompt_input_ids'][0], skip_special_tokens=True)
        print(f"Decoded Prompt (Text Part Only): {decoded_prompt}")
        print(f"Full Sequence Length to Gemma: {inputs['prompt_input_ids'].shape[1] + inputs['code_input_ids'].shape[1]} (Patches + Prompt Tokens)")
        print("--- End Input Debug ---\n")
    
    # --- 4. Generate Text ---
    print("\n--- Generating Description ---")
    print(f"Input Code File: {args.code_file}")
    
    generated_token_ids = []
    generated_text_so_far = ""
    unrelated_keywords = ['sine', 'cosine', 'random', 'python', 'optimizer']  # Add terms that shouldn't appear based on your dataset/code
    with torch.no_grad():
        for i in range(args.max_new_tokens):
            outputs = model(
                code_input_ids=inputs['code_input_ids'],
                code_attention_mask=inputs['code_attention_mask'],
                prompt_input_ids=inputs['prompt_input_ids'],
                prompt_attention_mask=inputs['prompt_attention_mask'],
                debug=args.debug,  # Pass debug flag to model
            )
            logits = outputs["logits"]
            
            # Get the logits for the very last token
            next_token_logits = logits[:, -1, :]

            # --- FIX: Clamp the logits to a reasonable range to prevent mode collapse ---
            next_token_logits = torch.clamp(next_token_logits, min=-30, max=30)

            # Apply repetition penalty
            if generated_token_ids:
                # Loop through unique generated tokens and apply the penalty
                for token_id in set(generated_token_ids):
                    if next_token_logits[0, token_id] > 0:
                        next_token_logits[0, token_id] /= args.repetition_penalty
                    else:
                        next_token_logits[0, token_id] *= args.repetition_penalty

            # Apply temperature and top-p sampling
            next_token_logits = next_token_logits / args.temperature
            probs = torch.nn.functional.softmax(next_token_logits, dim=-1)

            next_token_id = sample_top_p(probs, args.top_p)
            
            # Decode the token
            generated_token = processor.text_tokenizer.decode(next_token_id.item(), skip_special_tokens=True)
            
            if args.debug:
                print(f"Generated Token: {generated_token} (ID: {next_token_id.item()})")
            
            if next_token_id.item() == processor.text_tokenizer.eos_token_id or i >= 150:
                break
            generated_text_so_far += generated_token
            if i > 50 and generated_text_so_far[-50:] == generated_text_so_far[-100:-50]:
                print("Stopping early: Repetition detected.")
                break
            if i > 30 and any(kw in generated_text_so_far.lower() for kw in unrelated_keywords if kw not in code_string.lower()):
                print("Stopping early: Potential hallucination detected (unrelated keywords).")
                break
            
            generated_token_ids.append(next_token_id.item())
            
            # Append the new token to the prompt for the next iteration
            inputs["prompt_input_ids"] = torch.cat([inputs["prompt_input_ids"], next_token_id], dim=1)
            inputs["prompt_attention_mask"] = torch.cat(
                [inputs["prompt_attention_mask"], torch.ones_like(next_token_id)], dim=1
            )

    # --- 5. Decode and Print Output ---
    generated_text = processor.text_tokenizer.decode(generated_token_ids, skip_special_tokens=True)
    print(f"Generated Description: {prompt_text} {generated_text}")
    print("\n--- Inference Complete ---")

if __name__ == "__main__":
    main()
