import torch
import argparse

from modeling_codepatch import CodePatchConfig, CodeEncoderConfig, CodePatchForConditionalGeneration
from processing_codepatch import CodePatchProcessor

def get_args():
    parser = argparse.ArgumentParser(description="Run inference with a trained CodePatch model.")
    parser.add_argument("--checkpoint_path", type=str, required=True, help="Path to the model checkpoint .pt file.")
    parser.add_argument("--code_file", type=str, required=True, help="Path to the MATLAB code file.")
    parser.add_argument("--prompt", type=str, default="Describe the plot generated by this code:", help="The starting prompt for the model.")
    parser.add_argument("--max_new_tokens", type=int, default=100, help="Maximum number of new tokens to generate.")
    return parser.parse_args()

def main():
    args = get_args()
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    # --- 1. Re-create the exact model and processor configuration used for training ---
    code_encoder_config = CodeEncoderConfig(freeze_encoder=True)
    processor = CodePatchProcessor(
        "microsoft/codebert-base", "microsoft/codebert-base", code_encoder_config, device=device
    )
    text_config_dict = {
        "vocab_size": processor.text_tokenizer.vocab_size, "hidden_size": 2048, "intermediate_size": 8192,
        "num_hidden_layers": 12, "num_attention_heads": 16, "num_key_value_heads": 8, "max_position_embeddings": 4096,
    }
    full_config = CodePatchConfig(
        code_encoder_config=vars(code_encoder_config), text_config=text_config_dict,
        projection_dim=text_config_dict["hidden_size"], freeze_llm=True,
    )

    # --- 2. Instantiate the model and load the trained weights ---
    print("Instantiating model...")
    model = CodePatchForConditionalGeneration(full_config).to(device)
    
    print(f"Loading checkpoint from: {args.checkpoint_path}")
    checkpoint = torch.load(args.checkpoint_path, map_location=device)
    model.multi_modal_projector.load_state_dict(checkpoint['projector_state_dict'])
    model.code_encoder.position_embedding.load_state_dict(checkpoint['pos_embedding_state_dict'])
    model.eval() # Set model to evaluation mode
    print("Checkpoint loaded successfully.")

    # --- 3. Prepare Inputs ---
    print(f"Loading code from: {args.code_file}")
    with open(args.code_file, "r") as f:
        code_string = f.read()
    
    inputs = processor(code=code_string, text=args.prompt)
    
    # --- 4. Generate Text ---
    print("\n--- Generating Description ---")
    print(f"Prompt: {args.prompt}")
    
    generated_token_ids = []
    with torch.no_grad():
        for _ in range(args.max_new_tokens):
            outputs = model(**inputs)
            logits = outputs["logits"]
            
            # Get the most likely next token (greedy decoding)
            next_token_id = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(-1)
            
            # Stop if the model generates an EOS token
            if next_token_id.item() == processor.text_tokenizer.eos_token_id:
                break
            
            generated_token_ids.append(next_token_id.item())
            
            # Append the new token to the prompt for the next iteration
            inputs["prompt_input_ids"] = torch.cat([inputs["prompt_input_ids"], next_token_id], dim=1)
            inputs["prompt_attention_mask"] = torch.cat(
                [inputs["prompt_attention_mask"], torch.ones_like(next_token_id)], dim=1
            )

    # --- 5. Decode and Print Output ---
    generated_text = processor.text_tokenizer.decode(generated_token_ids, skip_special_tokens=True)
    print(f"Generated Text: {generated_text}")
    print("\n--- Inference Complete ---")

if __name__ == "__main__":
    main()
