import torch
import argparse
import warnings
import json
from tqdm import tqdm
from rouge_score import rouge_scorer
from transformers import GemmaForCausalLM, GemmaConfig, AutoTokenizer

warnings.filterwarnings("ignore", category=FutureWarning)

def get_args():
    parser = argparse.ArgumentParser(description="Run evaluation with a standard Gemma model.")
    parser.add_argument("--eval_dataset_path", type=str, required=True, help="Path to the evaluation dataset JSON file.")
    parser.add_argument("--output_path", type=str, required=True, help="Path to save the evaluation results.")
    parser.add_argument("--max_new_tokens", type=int, default=200, help="Maximum number of new tokens to generate.")
    parser.add_argument("--temperature", type=float, default=0.0, help="Temperature for sampling. 0 for greedy.")
    return parser.parse_args()

def main():
    args = get_args()
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    gemma_model_name = "google/gemma-2b"

    # --- 1. Instantiate the model and tokenizer ---
    print("Instantiating model and tokenizer...")
    model = GemmaForCausalLM.from_pretrained(gemma_model_name, torch_dtype=torch.bfloat16).to(device)
    tokenizer = AutoTokenizer.from_pretrained(gemma_model_name)
    
    # --- Get config values for KV cache calculation ---
    config = model.config
    num_layers = config.num_hidden_layers
    hidden_size = config.hidden_size
    bytes_per_param = 2 # for bfloat16

    model.eval()
    print("Model and tokenizer loaded successfully.")

    # --- 2. Load Evaluation Dataset ---
    print(f"Loading evaluation data from: {args.eval_dataset_path}")
    with open(args.eval_dataset_path, "r") as f:
        eval_dataset = json.load(f)

    results = []
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    for item in tqdm(eval_dataset, desc="Evaluating Gemma-2b"):
        code_string = item["code"]
        reference_description = item["description"]

        prompt_text = f"The following is a MATLAB script:\n\n```matlab\n{code_string}\n```\n\nDescribe the plot generated by this code, including shape, min/max, and key features:"
        inputs = tokenizer(prompt_text, return_tensors="pt").to(device)

        # --- 3. Calculate Input Metrics ---
        prompt_tokens = inputs['input_ids'].shape[1]
        total_input_tokens = prompt_tokens

        # --- KV Cache Calculation ---
        # For the standard Gemma model, the entire input (code + prompt) contributes to the KV cache.
        initial_kv_cache_tokens = prompt_tokens
        
        kv_cache_size_bytes = 2 * num_layers * hidden_size * initial_kv_cache_tokens * bytes_per_param
        kv_cache_size_mb = round(kv_cache_size_bytes / (1024**2), 2)

        # --- 4. Generate Text ---
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=args.max_new_tokens,
                temperature=args.temperature if args.temperature > 0 else None,
                do_sample=args.temperature > 0,
            )
            generated_ids = outputs[0]

        generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)

        # --- 5. Calculate ROUGE score ---
        scores = scorer.score(reference_description, generated_text)

        results.append({
            "code": code_string,
            "reference_description": reference_description,
            "generated_description": generated_text,
            "metrics": {
                "input_prompt_tokens": prompt_tokens,
                "total_input_constructs": total_input_tokens,
                "initial_kv_cache_tokens": initial_kv_cache_tokens,
                "kv_cache_size_mb": kv_cache_size_mb,
                "rouge1": scores['rouge1'].fmeasure,
                "rouge2": scores['rouge2'].fmeasure,
                "rougeL": scores['rougeL'].fmeasure
            }
        })

    # --- 6. Save Results ---
    print(f"Saving results to: {args.output_path}")
    with open(args.output_path, "w") as f:
        json.dump(results, f, indent=4)
    
    print("\n--- Evaluation Complete ---")

if __name__ == "__main__":
    main()
