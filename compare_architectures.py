import torch
import argparse
from transformers import GemmaForCausalLM, GemmaTokenizer, GemmaConfig
from modeling_codepatch import CodePatchConfig, CodeEncoderConfig, CodePatchForConditionalGeneration
from processing_codepatch import CodePatchProcessor
import os
import json
import matplotlib.pyplot as plt
from nltk.translate.bleu_score import sentence_bleu
import numpy as np
import re
from modeling_gemma import KVCache

def get_args():
    parser = argparse.ArgumentParser(description="Compare outputs from plain Gemma and CodePatch on MATLAB code.")
    parser.add_argument("--checkpoint_path", type=str, required=True, help="Path to CodePatch checkpoint .pt file.")
    parser.add_argument("--input_dir", type=str, help="Directory of .m files to process.")
    parser.add_argument("--dataset", type=str, default="philip120/RPOFES-dataset", help="Hugging Face dataset for comparison (with ground truth).")
    parser.add_argument("--output_file", type=str, default="comparison_results.json", help="File to save comparison results.")
    parser.add_argument("--max_new_tokens", type=int, default=200, help="Max tokens to generate.")
    parser.add_argument("--num_samples", type=int, default=10, help="Number of samples from dataset if using dataset mode.")
    parser.add_argument("--use_dataset", action="store_true", help="Use dataset instead of input_dir for inputs.")
    parser.add_argument("--local_dataset", type=str, help="Path to local JSON dataset file (overrides --use_dataset and --input_dir).")
    parser.add_argument("--plot", action="store_true", help="Compute BLEU scores and generate plots (requires ground truth from dataset).")
    parser.add_argument("--log_kv", action="store_true", help="Log KV cache token count and memory before/after generation.")
    return parser.parse_args()

def generate_with_plain_gemma(model, tokenizer, code_string, max_new_tokens, log_kv=False):
    device = next(model.parameters()).device
    dtype = next(model.parameters()).dtype
    
    prompt = f"Describe the plot generated by this MATLAB code, including shape, min/max, and key features:\n{code_string}"
    input_ids = tokenizer(prompt, return_tensors="pt")["input_ids"].to(device)
    
    if log_kv:
        kv_cache = KVCache()
        print("\nBefore Plain Gemma Generation (Prefill): KV Cache Tokens: 0, Memory: 0 MB")
    else:
        kv_cache = None
    
    # Prefill: Compute full
    seq_length = input_ids.shape[1]
    position_ids = torch.arange(seq_length, dtype=torch.long, device=device).unsqueeze(0)
    attention_mask_2d = torch.ones(1, seq_length, dtype=torch.bool, device=device)  # Change to bool
    causal_mask = torch.tril(torch.ones((seq_length, seq_length), dtype=torch.bool, device=device))
    attention_mask_4d = causal_mask[None, None, :, :] & attention_mask_2d[:, None, None, :]
    final_attention_mask = torch.zeros_like(attention_mask_4d, dtype=dtype)
    final_attention_mask.masked_fill_(~attention_mask_4d.to(torch.bool), torch.finfo(dtype).min)  # Cast to bool
    
    inputs_embeds = model.get_input_embeddings()(input_ids)
    outputs = model(inputs_embeds=inputs_embeds, attention_mask=final_attention_mask, position_ids=position_ids, kv_cache=kv_cache)
    next_token_logits = outputs["logits"][:, -1, :]
    next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
    
    if log_kv and kv_cache:
        token_count = kv_cache.num_items()
        memory_mb = sum(t.numel() * t.element_size() / (1024 ** 2) for cache in [kv_cache.key_cache, kv_cache.value_cache] for t in cache)
        print(f"After Prefill (Before Incremental): KV Cache Tokens: {token_count}, Memory: {memory_mb:.2f} MB")
    
    generated_ids = [next_token.item()]
    
    # Incremental generation
    current_pos = seq_length
    for i in range(1, max_new_tokens):
        if next_token.item() == tokenizer.eos_token_id:
            break
        
        new_inputs_embeds = model.get_input_embeddings()(next_token)
        new_position_ids = torch.tensor([[current_pos]], dtype=torch.long, device=device)
        
        # Create rectangular mask for q_len=1, kv_len = current kv +1
        kv_len = kv_cache.num_items() if kv_cache else 0
        causal_mask = torch.ones((1, kv_len + 1), dtype=torch.bool, device=device)  # Allow all for new token
        new_padding_mask_2d = torch.ones((1, 1), dtype=torch.bool, device=device)  # Change to bool
        attention_mask_4d = causal_mask[None, None, :, :] & new_padding_mask_2d[:, None, None, :]
        final_attention_mask = torch.zeros_like(attention_mask_4d, dtype=dtype)
        final_attention_mask.masked_fill_(~attention_mask_4d.to(torch.bool), torch.finfo(dtype).min)
        
        outputs = model(inputs_embeds=new_inputs_embeds, attention_mask=final_attention_mask, position_ids=new_position_ids, kv_cache=kv_cache)
        next_token_logits = outputs["logits"][:, -1, :]
        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
        generated_ids.append(next_token.item())
        current_pos += 1
    
    if log_kv and kv_cache:
        token_count = kv_cache.num_items()
        memory_mb = sum(t.numel() * t.element_size() / (1024 ** 2) for cache in [kv_cache.key_cache, kv_cache.value_cache] for t in cache)
        print(f"After Plain Gemma Generation: KV Cache Tokens: {token_count}, Memory: {memory_mb:.2f} MB")
    
    full_generated = tokenizer.decode(input_ids[0]) + tokenizer.decode(generated_ids, skip_special_tokens=True)
    return full_generated

def generate_with_codepatch(model, processor, code_string, max_new_tokens, log_kv=False):
    device = next(model.parameters()).device
    dtype = next(model.parameters()).dtype
    
    code_summary = ' '.join(code_string.splitlines()[:3])
    prompt_text = f"The MATLAB code is: {code_summary}... Describe the plot generated by this code, including shape, min/max, and key features:"
    inputs = processor(code=code_string, text=prompt_text)
    
    if log_kv:
        kv_cache = KVCache()
        print("\nBefore CodePatch Generation (Prefill): KV Cache Tokens: 0, Memory: 0 MB")
    else:
        kv_cache = None
    
    # Prefill
    outputs = model(**inputs, kv_cache=kv_cache)
    next_token_logits = outputs["logits"][:, -1, :]
    next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
    
    if log_kv and kv_cache:
        token_count = kv_cache.num_items()
        memory_mb = sum(t.numel() * t.element_size() / (1024 ** 2) for cache in [kv_cache.key_cache, kv_cache.value_cache] for t in cache)
        print(f"After Prefill (Before Incremental): KV Cache Tokens: {token_count}, Memory: {memory_mb:.2f} MB")
    
    generated_ids = [next_token.item()]
    
    # Incremental 
    current_pos = inputs["prompt_input_ids"].shape[1] + inputs["code_input_ids"].shape[1]  # Initial seq_len
    code_features = model.code_encoder(inputs["code_input_ids"], inputs["code_attention_mask"])
    projected_code = model.multi_modal_projector(code_features)
    for i in range(1, max_new_tokens):
        if next_token.item() == processor.text_tokenizer.eos_token_id:
            break
        
        new_prompt_input_ids = next_token
        new_prompt_embeds = model.language_model.get_input_embeddings()(new_prompt_input_ids)
        new_inputs_embeds = new_prompt_embeds
        
        new_position_ids = torch.tensor([[current_pos]], dtype=torch.long, device=device)
        
        kv_len = kv_cache.num_items() if kv_cache else 0
        causal_mask = torch.ones((1, kv_len + 1), dtype=torch.bool, device=device)
        new_padding_mask_2d = torch.ones((1, 1), dtype=torch.bool, device=device)  # To bool
        attention_mask_4d = causal_mask[None, None, :, :] & new_padding_mask_2d[:, None, None, :]
        final_attention_mask = torch.zeros_like(attention_mask_4d, dtype=projected_code.dtype)
        final_attention_mask.masked_fill_(~attention_mask_4d.to(torch.bool), torch.finfo(final_attention_mask.dtype).min)
        
        outputs = model.language_model(inputs_embeds=new_inputs_embeds, attention_mask=final_attention_mask, position_ids=new_position_ids, kv_cache=kv_cache)
        next_token_logits = outputs["logits"][:, -1, :]
        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
        generated_ids.append(next_token.item())
        current_pos += 1
    
    if log_kv and kv_cache:
        token_count = kv_cache.num_items()
        memory_mb = sum(t.numel() * t.element_size() / (1024 ** 2) for cache in [kv_cache.key_cache, kv_cache.value_cache] for t in cache)
        print(f"After CodePatch Generation: KV Cache Tokens: {token_count}, Memory: {memory_mb:.2f} MB")
    
    return processor.text_tokenizer.decode(generated_ids, skip_special_tokens=True)

def main():
    args = get_args()
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # Load plain Gemma using custom implementation with weights
    gemma_name = "google/gemma-2b"
    text_config = GemmaConfig.from_pretrained(gemma_name)
    gemma_model = GemmaForCausalLM(text_config).to(device)
    # Directly load state dict from HF (custom model should be compatible)
    from transformers import GemmaForCausalLM as HFGemma
    hf_model = HFGemma.from_pretrained(gemma_name, torch_dtype=torch.bfloat16)
    gemma_model.load_state_dict(hf_model.state_dict())
    gemma_tokenizer = GemmaTokenizer.from_pretrained(gemma_name)
    
    # Load CodePatch
    code_encoder_config = CodeEncoderConfig(freeze_encoder=True)
    processor = CodePatchProcessor("microsoft/codebert-base", gemma_name, code_encoder_config, device=device)
    text_config = GemmaConfig.from_pretrained(gemma_name)
    full_config = CodePatchConfig(code_encoder_config=vars(code_encoder_config), text_config=text_config.to_dict(), projection_dim=text_config.hidden_size, freeze_llm=True)
    codepatch_model = CodePatchForConditionalGeneration(full_config).to(device)
    codepatch_model.language_model.load_state_dict(gemma_model.state_dict())  # Share Gemma weights
    checkpoint = torch.load(args.checkpoint_path, map_location=device)
    codepatch_model.multi_modal_projector.load_state_dict(checkpoint['projector_state_dict'])
    codepatch_model.code_encoder.position_embedding.load_state_dict(checkpoint['pos_embedding_state_dict'])
    codepatch_model.eval()
    
    comparisons = []
    
    if args.local_dataset:
        print(f"Loading local dataset from: {args.local_dataset}")
        with open(args.local_dataset, "r") as f:
            dataset = json.load(f)
        samples = dataset[:args.num_samples] if args.num_samples else dataset  # Limit if specified
        for sample in samples:
            code = sample.get('code', sample.get('value'))  # Flexible key handling
            ground_truth = sample.get('description', sample.get('value'))  # Assume gpt is ground_truth
            plain_output = generate_with_plain_gemma(gemma_model, gemma_tokenizer, code, args.max_new_tokens, args.log_kv)
            codepatch_output = generate_with_codepatch(codepatch_model, processor, code, args.max_new_tokens, args.log_kv)
            comparisons.append({
                "code": code,
                "ground_truth": ground_truth,
                "plain_gemma": plain_output,
                "codepatch": codepatch_output
            })
    elif args.use_dataset:
        from datasets import load_dataset
        dataset = load_dataset(args.dataset, split="test")  # Assume test split exists
        samples = dataset.shuffle().select(range(args.num_samples))
        for sample in samples:
            code = sample['code']  # Adjust keys based on your dataset
            ground_truth = sample['description']
            plain_output = generate_with_plain_gemma(gemma_model, gemma_tokenizer, code, args.max_new_tokens, args.log_kv)
            codepatch_output = generate_with_codepatch(codepatch_model, processor, code, args.max_new_tokens, args.log_kv)
            comparisons.append({
                "code": code,
                "ground_truth": ground_truth,
                "plain_gemma": plain_output,
                "codepatch": codepatch_output
            })
    else:
        for file_name in os.listdir(args.input_dir):
            if file_name.endswith(".m"):
                file_path = os.path.join(args.input_dir, file_name)
                with open(file_path, "r") as f:
                    code = f.read()
                plain_output = generate_with_plain_gemma(gemma_model, gemma_tokenizer, code, args.max_new_tokens, args.log_kv)
                codepatch_output = generate_with_codepatch(codepatch_model, processor, code, args.max_new_tokens, args.log_kv)
                comparisons.append({
                    "file": file_name,
                    "code": code,
                    "plain_gemma": plain_output,
                    "codepatch": codepatch_output
                })
    
    with open(args.output_file, "w") as f:
        json.dump(comparisons, f, indent=4)
    
    print(f"Comparisons saved to {args.output_file}. You can evaluate quality manually or add metrics like BLEU.")

    if (args.use_dataset or args.local_dataset) and args.plot:
        print("\nComputing BLEU scores and generating plots...")
        plain_bleu_scores = []
        codepatch_bleu_scores = []
        for comp in comparisons:
            reference = [comp["ground_truth"].split()]  # Tokenize reference
            plain_candidate = comp["plain_gemma"].split()
            codepatch_candidate = comp["codepatch"].split()
            plain_bleu = sentence_bleu(reference, plain_candidate)
            codepatch_bleu = sentence_bleu(reference, codepatch_candidate)
            plain_bleu_scores.append(plain_bleu)
            codepatch_bleu_scores.append(codepatch_bleu)
        
        # Average scores
        avg_plain = np.mean(plain_bleu_scores)
        avg_codepatch = np.mean(codepatch_bleu_scores)
        print(f"Average BLEU - Plain Gemma: {avg_plain:.4f}")
        print(f"Average BLEU - CodePatch: {avg_codepatch:.4f}")
        
        # Bar plot of averages
        plt.figure(figsize=(8, 6))
        models = ['Plain Gemma', 'CodePatch']
        avgs = [avg_plain, avg_codepatch]
        plt.bar(models, avgs, color=['blue', 'green'])
        plt.title('Average BLEU Scores')
        plt.ylabel('BLEU Score')
        plt.ylim(0, 1)
        plt.savefig('average_bleu_comparison.png')
        plt.close()
        
        # Scatter plot of per-sample scores
        plt.figure(figsize=(10, 6))
        x = np.arange(len(plain_bleu_scores))
        plt.scatter(x, plain_bleu_scores, color='blue', label='Plain Gemma')
        plt.scatter(x, codepatch_bleu_scores, color='green', label='CodePatch')
        plt.title('Per-Sample BLEU Scores')
        plt.xlabel('Sample Index')
        plt.ylabel('BLEU Score')
        plt.legend()
        plt.savefig('per_sample_bleu_comparison.png')
        plt.close()
        
        print("Plots saved: average_bleu_comparison.png and per_sample_bleu_comparison.png")

        # Custom Metrics: Parse structured descriptions and compare fields
        print("\nComputing custom metrics...")
        metrics = [
            "plot shape", "maximum", "minimum", "pivot points", "direction",
            "x-intercepts", "y-intercepts", "final value", "overshoot",
            "other notable features"  # 10th is a catch-all, could split if needed
        ]
        plain_metric_scores = {m: [] for m in metrics}
        codepatch_metric_scores = {m: [] for m in metrics}
        
        def parse_description(desc):
            # Parse format like "[key: value, key: value, ...]"
            parsed = {}
            if desc.startswith('[') and desc.endswith(']'):
                content = desc[1:-1]
                pairs = re.split(r',\s*(?![^()]*\))', content)  # Split on commas not inside parens
                for pair in pairs:
                    if ':' in pair:
                        key, value = pair.split(':', 1)
                        parsed[key.strip()] = value.strip()
            return parsed
        
        for comp in comparisons:
            gt_parsed = parse_description(comp["ground_truth"])
            plain_parsed = parse_description(comp["plain_gemma"])
            codepatch_parsed = parse_description(comp["codepatch"])
            
            for metric in metrics:
                gt_val = gt_parsed.get(metric, "")
                plain_val = plain_parsed.get(metric, "")
                codepatch_val = codepatch_parsed.get(metric, "")
                
                # Simple exact match (could refine: e.g., numerical tolerance for min/max)
                plain_match = 1 if plain_val == gt_val and gt_val else 0
                codepatch_match = 1 if codepatch_val == gt_val and gt_val else 0
                
                plain_metric_scores[metric].append(plain_match)
                codepatch_metric_scores[metric].append(codepatch_match)
        
        # Average per metric
        avg_plain_metrics = {m: np.mean(scores) for m, scores in plain_metric_scores.items()}
        avg_codepatch_metrics = {m: np.mean(scores) for m, scores in codepatch_metric_scores.items()}
        
        print("Average Custom Metric Accuracies:")
        for m in metrics:
            print(f"{m}: Plain Gemma={avg_plain_metrics[m]:.4f}, CodePatch={avg_codepatch_metrics[m]:.4f}")
        
        # Bar plot for custom metrics
        plt.figure(figsize=(12, 8))
        x = np.arange(len(metrics))
        width = 0.35
        plt.bar(x - width/2, [avg_plain_metrics[m] for m in metrics], width, label='Plain Gemma', color='blue')
        plt.bar(x + width/2, [avg_codepatch_metrics[m] for m in metrics], width, label='CodePatch', color='green')
        plt.xlabel('Metrics')
        plt.ylabel('Average Accuracy')
        plt.title('Average Accuracy per Custom Metric')
        plt.xticks(x, metrics, rotation=45, ha='right')
        plt.legend()
        plt.tight_layout()
        plt.savefig('custom_metrics_comparison.png')
        plt.close()
        
        print("Custom metrics plot saved: custom_metrics_comparison.png")

if __name__ == "__main__":
    main()
